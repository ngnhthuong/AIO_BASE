{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MLP for Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhatthuong/.miniconda3/envs/env_vqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST(\n",
    "    root = '/home/nhatthuong/Documents/ResearchVQA-VQG-firstpaper/AIO/MultilayerPerceptron/data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform=transform\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root = '/home/nhatthuong/Documents/ResearchVQA-VQG-firstpaper/AIO/MultilayerPerceptron/data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=1024, num_workers=2, shuffle=True, drop_last=True)\n",
    "testloader = DataLoader(testset, batch_size=1024, num_workers=2 ,shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (value, labels) in enumerate(trainloader):\n",
    "        print(f'Batch {i+1}, ----{value.shape}, --- {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhatthuong/.miniconda3/envs/env_vqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.FashionMNIST(\n",
    "    root = '/home/nhatthuong/Documents/ResearchVQA-VQG-firstpaper/AIO/MultilayerPerceptron/data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=1024, num_workers=2, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size is 1 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 2 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 3 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 4 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 5 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 6 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 7 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 8 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 9 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 10 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 11 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 12 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 13 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 14 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 15 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 16 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 17 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 18 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 19 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 20 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 21 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 22 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 23 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 24 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 25 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 26 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 27 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 28 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 29 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 30 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 31 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 32 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 33 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 34 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 35 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 36 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 37 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 38 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 39 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 40 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 41 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 42 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 43 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 44 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 45 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 46 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 47 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 48 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 49 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 50 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 51 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 52 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 53 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 54 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 55 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 56 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 57 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n",
      "Batch size is 58 with value torch.Size([1024, 1, 28, 28]), labels torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for i, (values, labels) in enumerate(trainloader):\n",
    "    print(f'Batch size is {i+1} with value {values.shape}, labels {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = torch.tensor([[[[1,1],[2,2]],\n",
    "                    [[1,1],[2,2]],\n",
    "                    [[1,1],[2,2]]],\n",
    "                    [[[1,1],[2,2]],\n",
    "                    [[1,1],[2,2]],\n",
    "                    [[1,1],[2,2]]],\n",
    "                    [[[1,1],[2,2]],\n",
    "                    [[1,1],[2,2]],\n",
    "                    [[1,1],[2,2]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([0.3001], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(conv_layer.weight.shape)\n",
    "print(conv_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn((1,7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.2432e-01, -2.7113e-01, -2.2541e+00,  2.9326e-01,  6.4404e-01,\n",
       "           1.7844e+00, -3.3138e-02],\n",
       "         [-1.2946e+00,  7.6437e-01,  1.0863e+00, -2.2195e+00,  5.2445e-01,\n",
       "          -4.4500e-01, -1.3832e+00],\n",
       "         [-7.1121e-01, -1.0245e+00, -7.5580e-03, -6.4591e-01, -1.8293e+00,\n",
       "           5.1419e-02,  5.1846e-01],\n",
       "         [ 1.2814e+00, -1.3596e+00, -1.2580e-01, -1.2021e+00, -5.9586e-01,\n",
       "          -3.9395e-01, -1.4032e+00],\n",
       "         [-4.0411e-01,  7.0491e-01, -2.6779e-03,  8.9319e-03,  7.2446e-01,\n",
       "           2.6648e+00, -9.0186e-01],\n",
       "         [ 1.3493e+00, -4.8652e-01, -8.6847e-02, -8.1965e-01,  2.0118e+00,\n",
       "          -2.8315e-01, -1.1702e-01],\n",
       "         [ 1.2449e+00,  7.3380e-01,  5.4550e-01,  1.5340e-01,  3.6370e-01,\n",
       "           2.6879e+00, -3.9652e-01]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(conv_layer, nn.ReLU())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1              [-1, 1, 5, 5]              10\n",
      "              ReLU-2              [-1, 1, 5, 5]               0\n",
      "================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "output = model(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = nn.Flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
